---
title: "Final Group project"
author: "Your name goes here"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
  pdf_document:
    toc: yes
  word_document:
    toc: yes
---

```{r}
#| label: load-libraries
#| echo: false # This option disables the printing of code (only output is displayed).
#| message: false
#| warning: false


library(tidyverse)
library(tidymodels)
library(skimr)
library(lubridate)
library(kknn)
library(here)
library(tictoc)
library(vip)
library(ranger)
library(tidygeocoder)
library(sf)
library(mapview)
library(ggplot2)
library(GGally)

options(scipen = 999)

```

# The problem: predicting credit card fraud

The goal of the project is to predict fraudulent credit card transactions.

We will be using a dataset with credit card transactions containing legitimate and fraud transactions. Fraud is typically well below 1% of all transactions, so a naive model that predicts that all transactions are legitimate and not fraudulent would have an accuracy of well over 99%-- pretty good, no?

You can read more on credit card fraud on [Credit Card Fraud Detection Using Weighted Support Vector Machine](https://www.scirp.org/journal/paperinformation.aspx?paperid=105944)

The dataset we will use consists of credit card transactions and it includes information about each transaction including customer details, the merchant and category of purchase, and whether or not the transaction was a fraud.

## Obtain the data

The dataset is too large to be hosted on Canvas or Github, so please download it from dropbox <https://www.dropbox.com/sh/q1yk8mmnbbrzavl/AAAxzRtIhag9Nc_hODafGV2ka?dl=0> and save it in your `dsb` repo, under the `data` folder.

As we will be building a classifier model using tidymodels, there's two things we need to do:

1.  Define the outcome variable `is_fraud` as a factor, or categorical, variable, instead of the numerical 0-1 varaibles.
2.  In tidymodels, the first level is the event of interest. If we leave our data as is, `0` is the first level, but we want to find out when we actually did (`1`) have a fraudulent transaction

```{r}
#| echo: false
#| message: false
#| warning: false

card_fraud <- read_csv(here::here("data", "card_fraud.csv")) %>% 

  mutate(
    # in tidymodels, outcome should be a factor  
    is_fraud = factor(is_fraud),
    
    # first level is the event in tidymodels, so we need to reorder
    is_fraud = relevel(is_fraud, ref = "1")
         )

glimpse(card_fraud)
```

The data dictionary is as follows

| column(variable)      | description                                 |
|-----------------------|---------------------------------------------|
| trans_date_trans_time | Transaction DateTime                        |
| trans_year            | Transaction year                            |
| category              | category of merchant                        |
| amt                   | amount of transaction                       |
| city                  | City of card holder                         |
| state                 | State of card holder                        |
| lat                   | Latitude location of purchase               |
| long                  | Longitude location of purchase              |
| city_pop              | card holder's city population               |
| job                   | job of card holder                          |
| dob                   | date of birth of card holder                |
| merch_lat             | Latitude Location of Merchant               |
| merch_long            | Longitude Location of Merchant              |
| is_fraud              | Whether Transaction is Fraud (1) or Not (0) |

We also add some of the variables we considered in our EDA for this dataset during homework 2.

```{r}

library(lubridate)

#converting the single transaction date/time column into individual columns for hour, weekday, month and age of cardholder
card_fraud <- card_fraud %>% 
  mutate( hour = hour(trans_date_trans_time),
          wday = wday(trans_date_trans_time, label = TRUE),
          month_name = month(trans_date_trans_time, label = TRUE),
          age = interval(dob, trans_date_trans_time) / years(1) #determining age of cardholder by taking difference between date of birth and transaction date
) %>% 
  
  mutate(
    
    # convert latitude/longitude to radians
    lat1_radians = lat / 57.29577951,
    lat2_radians = merch_lat / 57.29577951,
    long1_radians = long / 57.29577951,
    long2_radians = merch_long / 57.29577951,
    
    # calculate distance in miles
    distance_miles = 3963.0 * acos((sin(lat1_radians) * sin(lat2_radians)) + cos(lat1_radians) * cos(lat2_radians) * cos(long2_radians - long1_radians)),

    # calculate distance in km
    distance_km = 6377.830272 * acos((sin(lat1_radians) * sin(lat2_radians)) + cos(lat1_radians) * cos(lat2_radians) * cos(long2_radians - long1_radians))

  )

glimpse(card_fraud)

```


# Exploratory Data Analysis (EDA)

You have done some EDA and you can pool together your group's expertise in which variables to use as features. You can reuse your EDA from earlier, but we expect at least a few visualisations and/or tables to explore the dataset and identify any useful features.

Group all variables by type and examine each variable class by class. The dataset has the following types of variables:

1.  Strings
2.  Geospatial Data
3.  Dates
4.  Date/Times
5.  Numerical

Strings are usually not a useful format for classification problems. The strings should be converted to factors, dropped, or otherwise transformed.

## Scatterplot - Correlation Matrix


```{r, warning=FALSE, message=FALSE}

#plotting the correlation matrix of the dataframe 
card_fraud %>% 
  slice_sample(prop = 0.01) %>%
  select(is_fraud, amt, city_pop, distance_km, age, lat, long )%>% #selecting the required variables
  ggpairs(aes(colour=is_fraud), alpha=0.2)+
  theme_bw()
```

The dataframe is too large to plot it fully so we slice 1% of it and analyze the correlations. On first sight, no variables seem to have any meaningful correlations to each other, but the differences in correlation between fraud and non-fraud categories is quite interesting. For example when there is fraud, city popoulation and transaction amount are much more correlated than if there isn't fraud. An observation we make is that the average transaction amount for fraud transactions is signifficantly higher, although the max amount is higher for non-fraud transactions. 


## Factorization and simple sumary statistics

Strings are usually not a useful format for classification problems. The strings should be converted to factors, dropped, or otherwise transformed.

```{r, warning=FALSE}
card_fraud$trans_date_trans_time <- as.POSIXct(card_fraud$trans_date_trans_time)




# Convert category, city, and state to factors
card_fraud$category <- as.factor(card_fraud$category)
card_fraud$city <- as.factor(card_fraud$city)
card_fraud$state <- as.factor(card_fraud$state)

card_fraud$category <- factor(card_fraud$category, levels = names(sort(table(card_fraud$category))))

#card_fraud <- card_fraud %>% mutate(category = factor(category, 
                                                    #  levels = names(sort(table(category)))),
                                   # city = as.factor(city), 
                                   # state = as.factor(state))

# Summary statistics of transaction amounts by category
amount_summary <- card_fraud %>%
  group_by(category) %>%
  summarise(
    count = n(),
    mean_amount = mean(amt),
    sd_amount = sd(amt)
  )

amount_summary

# Bar plot of transaction counts by category
ggplot(card_fraud, aes(x = category)) +
  geom_bar(stat = "count", fill = "purple") +
  geom_text(
    aes(label = stat(count)),
    stat = 'count',
    vjust = -0.5,
    size = 3,
    color = 'black'
  ) +
  labs(title = "Most card transctions happen at gas stations and grocery stores", subtitle = 'Transaction Counts by Merchant Category', x = 'Category', y = 'Count') +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


# Table of average transaction amounts by state
state_avg_amount <- card_fraud %>%
  group_by(state) %>%
  summarise(avg_amount = mean(amt)) %>%
  arrange(desc(avg_amount))
print(state_avg_amount)


# Scatter plot of transaction locations based on latitude and longitude
ggplot(card_fraud, aes(x = long, y = lat, color = is_fraud)) +
  geom_point() +
  labs(title = 'Transaction Locations are spread more or less 
       evenly across the US, \n although there are more in the 
       Eastern side', x = 'Longitude', y = 'Latitude')

```

The first summary table shows that the travel category has the highest average transaction amount (114.30), and a standard deviation of 622.91. There are 20,873 count values, so it is not the most frequent amount. The second highest mean transaction amount is for groceries (53.65), and a standard deviation which is much lower, (22.69). This makes sense considering that travel tends to be much more expensive and people vary a lot in the amount they spend, whereas grocery is a more frequent purchase of a lesser value and people tend to spend more or less the same.

The most frequent transaction belongs to the gas_transport category, followed by grocery (68046 and 63791, respectively). This makes sense considering they are purchases people make very frequently.

The states with the highest average transaction amounts are Delaware, Rhode Island and Vermont (476.77, 100.96 and 83.15, respectively).

Most transactions are cluttered between the longitudes -130 and -60 and latitudes 25 and 50. There seems to be no effect on whether these transactions are fraud or not. There are very few outliers.

## Variable distribution

```{r, variable normality}
#Assessing the normality

my_card_fraud <- card_fraud %>% 
  
  slice_sample(prop = 0.10) 

# Create a vector of variable names you want to assess for normality
variables <- c("amt", "long", "city_pop", "merch_lat", "merch_long", 
               "hour", "age", "lat1_radians", "lat2_radians", "long1_radians", 
               "long2_radians", "distance_miles", "distance_km")

# Loop through the variables and create Q-Q plots
for (var in variables) {
  qqnorm(my_card_fraud[[var]], main = paste("Q-Q Plot of", var))
  qqline(my_card_fraud[[var]], col = "red")
}
```

After assesing the distribution of our number variables, we have come to the conclusion that transaction amount and city population do not follow a normal distribution but rather an exponetial increase and therefore, we will transfor the scales of these variables to logarithmic in the recipes.

## Variable outliers

```{r}
# Create a vector of variable names you want to include in the boxplots
variables <- c("trans_year", "amt", "lat", "long", "city_pop", "merch_lat", "merch_long", 
               "hour", "age", "lat1_radians", "lat2_radians", "long1_radians", 
               "long2_radians", "distance_miles", "distance_km")

# Create a subset of the data with the selected variables
subset_df <- my_card_fraud[, variables]

# Set up the plotting environment
par(mfrow = c(4, 4))  
par(mar = c(2, 2, 1, 1))  

# Create boxplots for each variable
for (i in 1:length(variables)) {
  boxplot(subset_df[, i], main = variables[i], ylab = "Value")
}
```

The variables with most outliers are city population and transaction amount. The outliers have very high values that prevent us from appreciating at the distribution. Distance, age and hour seem to have the least outliers. This analysis reaffirs our decision to change the scale for transaction amount and city population.


***Strings to Geospatial Data***

We have plenty of geospatial data as lat/long pairs, so I want to convert city/state to lat/long so I can compare to the other geospatial variables. This will also make it easier to compute new variables like the distance the transaction is from the home location.

-   `city`, City of Credit Card Holder
-   `state`, State of Credit Card Holder

## Exploring factors: how is the compactness of categories?

-   Do we have excessive number of categories? Do we want to combine some?

```{r, job and category count}
card_fraud %>% 
  count(category, sort=TRUE)%>% 
  mutate(perc = n/sum(n))

card_fraud %>% 
  count(job, sort=TRUE) %>% 
  mutate(perc = n/sum(n)) 
  

```

Based on these proportions, we think it would be wise to cutoff categories at proportions under 5% and jobs under 0.1%, collapsing 3 and 128 variables respectively into "other". This step will be done in the recipe to preserve continuity.

The predictors `category` and `job` are transformed into factors.

```{r}
#| label: convert-strings-to-factors


card_fraud <- card_fraud %>% 
  mutate(category = factor(category),
         job = factor(job))

glimpse(card_fraud)
```

`category` has 14 unique values, and `job` has 494 unique values. The dataset is quite large, with over 670K records, so these variables don't have an excessive number of levels at first glance. However, it is worth seeing if we can compact the levels to a smaller number.

### Why do we care about the number of categories and whether they are "excessive"?

Consider the extreme case where a dataset had categories that only contained one record each. There is simply insufficient data to make correct predictions using category as a predictor on new data with that category label. Additionally, if your modeling uses dummy variables, having an extremely large number of categories will lead to the production of a huge number of predictors, which can slow down the fitting. This is fine if all the predictors are useful, but if they aren't useful (as in the case of having only one record for a category), trimming them will improve the speed and quality of the data fitting.

If I had subject matter expertise, I could manually combine categories. If you don't have subject matter expertise, or if performing this task would be too labor intensive, then you can use cutoffs based on the amount of data in a category. If the majority of the data exists in only a few categories, then it might be reasonable to keep those categories and lump everything else in an "other" category or perhaps even drop the data points in smaller categories.

## Do all variables have sensible types?

Consider each variable and decide whether to keep, transform, or drop it. This is a mixture of Exploratory Data Analysis and Feature Engineering, but it's helpful to do some simple feature engineering as you explore the data. In this project, we have all data to begin with, so any transformations will be performed on the entire dataset. Ideally, do the transformations as a `recipe_step()` in the tidymodels framework. Then the transformations would be applied to any data the recipe was used on as part of the modeling workflow. There is less chance of data leakage or missing a step when you perform the feature engineering in the recipe.

## Which variables to keep in your model?

You have a number of variables and you have to decide which ones to use in your model. For instance, you have the latitude/lognitude of the customer, that of the merchant, the same data in radians, as well as the `distance_km` and `distance_miles`. Do you need them all?

###Unnecessary variables we will drop

### Is the transaction year a good predictor?

```{r, assessing year variable as sensible predictor}
card_fraud %>% 
  group_by(trans_year) %>% 
  count(is_fraud, sort=TRUE) %>% 
  mutate(perc = n/sum(n)) 
```

We are trying to asses whether we need the transaction year as a predictor for our model. In 2019 The incidence of fraud was 0.57%, while it was 0.63% in 2020. As, the incidence barely changes from year to year and as our transcation year variable can only either take 2019 or 2020 as a value, we predict that this variable will be a very poor predictor and will drop it during feature selection in the recipes.


# Model Fit - 1st Iteration

## Fit your workflows in smaller sample

You will be running a series of different models, along the lines of the California housing example we have seen in class. However, this dataset has 670K rows and if you try various models and run cross validation on them, your computer may slow down or crash.

Thus, we will work with a smaller sample of 10% of the values the original dataset to identify the best model, and once we have the best model we can use the full dataset to train- test our best model.

```{r, subset selection}
# select a smaller subset
my_card_fraud <- card_fraud %>% 
  
  slice_sample(prop = 0.10) # 10% of the entire dataframe 
```

## Split the data in training - testing

```{r, initial data split}
# **Split the data**

set.seed(123)

data_split <- initial_split(my_card_fraud, # updated data
                           prop = 0.8, 
                           strata = is_fraud)

card_fraud_train <- training(data_split) 
card_fraud_test <- testing(data_split)
```

## Cross Validation

Start with 3 CV folds to quickly get an estimate for the best model and you can increase the number of folds to 5 or 10 later.

```{r}
set.seed(123)
cv_folds <- vfold_cv(data = card_fraud_train, 
                          v = 3, 
                          strata = is_fraud)
cv_folds 
```

## Define a tidymodels `recipe`

What steps are you going to add to your recipe? Do you need to do any log transformations?

```{r, define_recipe}

fraud_rec <- recipe(is_fraud ~ ., data = card_fraud_train) %>%
  
  #drop columns identified during EDA to not be relevant or unnecessary
  step_rm(trans_year, trans_date_trans_time, dob, lat1_radians, lat2_radians, long1_radians, long2_radians, distance_miles, city, state, lat, long, merch_lat, merch_long) %>% 
  
  #collapses all merchant categories under 5% to other
  step_other(category, threshold = .05) %>% 
  
  #collapses all job categories under 0.1% to other
  step_other(job, threshold = .003) %>% 
  
  #Changes the scale of the selected columns to log scale
  step_log(amt, city_pop) %>% 
  
  #Adds a catch-all level to a factor for any new values not encountered in model training, which lets R intelligently predict new levels in the test set.
  step_novel(all_nominal(), -all_outcomes()) %>% # Use *before* `step_dummy()` so new level is dummified
  
  #Converts nominal data into numeric dummy variables
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  
  #Intelligently handles zero variance variables (variables that contain only a single value)
  step_zv(all_numeric(), -all_outcomes())  %>% 
  
  #Centers then scales numeric variable (mean = 0, sd = 1)
  step_normalize(all_numeric()) 



```

Once you have your recipe, you can check the pre-processed dataframe

```{r}
 prepped_data <- 
   fraud_rec %>% # use the recipe object
   prep() %>% # perform the recipe on training data
   juice() # extract only the preprocessed dataframe 
 
 glimpse(prepped_data)

```

## Define various models

You should define the following classification models:

1.  Logistic regression, using the `glm` engine
2.  Decision tree, using the `C5.0` engine
3.  Random Forest, using the `ranger` engine and setting `importance = "impurity"`)\
4.  A boosted tree using Extreme Gradient Boosting, and the `xgboost` engine
5.  A k-nearest neighbours, using 4 nearest_neighbors and the `kknn` engine

```{r, define_models, warning=FALSE, error=TRUE}
## Model Building 

# Logistic regression
log_spec <-  logistic_reg() %>%  # model type
  set_engine(engine = "glm") %>%  # model engine
  set_mode("classification") # model mode

# Show your model specification
log_spec

# Decision Tree
tree_spec <- decision_tree() %>%
  set_engine(engine = "C5.0") %>%
  set_mode("classification")

tree_spec

# Random Forest
library(ranger)

rf_spec <- 
  rand_forest() %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")


# Boosted tree (XGBoost)
library(xgboost)

xgb_spec <- 
  boost_tree() %>% 
  set_engine("xgboost") %>% 
  set_mode("classification") 

# K-nearest neighbour (k-NN)
knn_spec <- 
  nearest_neighbor(neighbors = 4) %>% # we can adjust the number of neighbors 
  set_engine("kknn") %>% 
  set_mode("classification") 
```

## Bundle recipe and model with `workflows`

```{r, define_workflows}

## Bundle recipe and model with `workflows`

log_wflow <- # new workflow object
 workflow() %>% # use workflow function
 add_recipe(fraud_rec) %>%   # use the new recipe
 add_model(log_spec)   # add your model spec

# show object
log_wflow



## A few more workflows

tree_wflow <-
 workflow() %>%
 add_recipe(fraud_rec) %>% 
 add_model(tree_spec) 

rf_wflow <-
 workflow() %>%
 add_recipe(fraud_rec) %>% 
 add_model(rf_spec) 

xgb_wflow <-
 workflow() %>%
 add_recipe(fraud_rec) %>% 
 add_model(xgb_spec)

knn_wflow <-
 workflow() %>%
 add_recipe(fraud_rec) %>% 
 add_model(knn_spec)
```

## Fit models

You may want to compare the time it takes to fit each model. `tic()` starts a simple timer and `toc()` stops it

```{r, fit_models}

#logistic regression
 tic()
 log_res <- log_wflow %>% 
   fit_resamples(
     resamples = cv_folds, 
     metrics = metric_set(
       recall, precision, f_meas, accuracy,
       kap, roc_auc, sens, spec),
     control = control_resamples(save_pred = TRUE)) 
 time <- toc()
 log_time <- time[[4]]

 #decision tree
 #tic()
 #tree_res <- tree_wflow %>% 
 #  fit_resamples(
 #    resamples = cv_folds, 
#     metrics = metric_set(
#       recall, precision, f_meas, accuracy,
#       kap, roc_auc, sens, spec),
#     control = control_resamples(save_pred = TRUE)) 
# time <- toc()
# tree_time <- time[[4]]
 
 #random forest
 tic()
 rf_res <- rf_wflow %>% 
   fit_resamples(
     resamples = cv_folds, 
     metrics = metric_set(
       recall, precision, f_meas, accuracy,
       kap, roc_auc, sens, spec),
     control = control_resamples(save_pred = TRUE)) 
 time <- toc()
 rf_time <- time[[4]]
 
 # Boosted tree (XGBoost)
 tic()
 xgb_res <- xgb_wflow %>% 
   fit_resamples(
     resamples = cv_folds, 
     metrics = metric_set(
       recall, precision, f_meas, accuracy,
       kap, roc_auc, sens, spec),
     control = control_resamples(save_pred = TRUE)) 
 time <- toc()
 xgb_time <- time[[4]]
 


# K-nearest neighbour (k-NN)
# tic()
# knn_res <- knn_wflow %>% 
#   fit_resamples(
#     resamples = cv_folds, 
#     metrics = metric_set(
#       recall, precision, f_meas, accuracy,
#       kap, roc_auc, sens, spec),
#     control = control_resamples(save_pred = TRUE)) 
# time <- toc()
# knn_time <- time[[4]]

```

## Compare models

```{r, compare_models}
## Model Comparison

 log_metrics <- 
  log_res %>% 
  collect_metrics(summarise = TRUE) %>%
  # add the name of the model to every row
  mutate(model = "Logistic Regression",
          time = log_time) 

#tree_metrics <- 
#  tree_res %>% 
#  collect_metrics(summarise = TRUE) %>%
#  mutate(model = "Decision Tree",
#          time = tree_time)

rf_metrics <- 
  rf_res %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "Random Forest",
          time = rf_time)

xgb_metrics <- 
  xgb_res %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "XGBoost",
          time = xgb_time)

#knn_metrics <- 
#  knn_res %>% 
#  collect_metrics(summarise = TRUE) %>%
#  mutate(model = "Knn",
#          time = knn_time)

# create dataframe with all models
model_compare <- bind_rows(log_metrics,
                           
                           rf_metrics,
                           xgb_metrics) %>% 
   # get rid of 'sec elapsed' and turn it into a number
   mutate(time = str_sub(time, end = -13) %>% 
            as.double()
          )


#Pivot wider to create barplot
  model_comp <- model_compare %>% 
  select(model, .metric, mean, std_err) %>% 
  pivot_wider(names_from = .metric, values_from = c(mean, std_err)) 

# show mean are under the curve (ROC-AUC) for every model
model_comp %>% 
  arrange(mean_roc_auc) %>% 
  mutate(model = fct_reorder(model, mean_roc_auc)) %>% # order results
  ggplot(aes(model, mean_roc_auc, fill=model)) +
  geom_col() +
  coord_flip() +
  scale_fill_brewer(palette = "Blues") +
   geom_text(
     size = 3,
     aes(label = round(mean_roc_auc, 2), 
         y = mean_roc_auc + 0.08),
     vjust = 1
  )+
  theme_light()+
  theme(legend.position = "none")+
  labs(y = NULL)
 
 
```

The Boosted Tree odel seems to have the best roc_auc out of all models at 0.98 and it also computes the fastest at a runtime of 20.56s. Therefore, for our final model we will proceed with XGB. Normal tree and knn have runtimes that are way too large and will be deactivated for knitting.

## Which metric to use

In the context of fraud detection we want to ake sure that the number of false negatives is the lwoest as the cost for that is higher than false positives for banks that's why we focus on recall. It measures the proportion of actual positive instances (fraudulent transactions) that are correctly identified as positive by the model. Maximizing recall helps minimize false negatives, ensuring that fraudulent transactions are detected and mitigating potential financial losses

```{r}

model_comp

```
Out of all models, XGB has the highest recall and will therefore create the best results.


# Model Fit - Final Iteration

## Splitting full dataset

```{r, full data split}
# **Split the data**

set.seed(123)

data_split_full <- initial_split(card_fraud, # updated data
                           prop = 0.8, 
                           strata = is_fraud)

#card_fraud_train <- training(data_split) 
#card_fraud_test <- testing(data_split)
```

## Last fit with full dataset

```{r, last fit}

## `last_fit()` on test set

# - `last_fit()`  fits a model to the whole training data and evaluates it on the test set. 
# - provide the workflow object of the best model as well as the data split object (not the training data). 
 
last_fit_xgb <- last_fit(xgb_wflow, 
                        split = data_split_full,
                        metrics = metric_set(
                          accuracy, f_meas, kap, precision,
                          recall, roc_auc, sens, spec))

last_fit_xgb %>% collect_metrics(summarize = TRUE)

#Compare to training
xgb_res %>% collect_metrics(summarize = TRUE)

```

With a recall of 0.754, we have a great model to predict with.


## Get variable importance using `vip` package

```{r, variable importance final}

library(vip)

last_fit_xgb %>% 
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  vip(num_features = 10) +
  theme_light()

```

As expected, transaction aount is the most important variable.

## Plot Final Confusion matrix and ROC curve

```{r, final confusion matrix and roc curve}
## Final Confusion Matrix

 last_fit_xgb %>%
   collect_predictions() %>% 
   conf_mat(is_fraud, .pred_class) %>% 
   autoplot(type = "heatmap")
 
 
## Final ROC curve
 last_fit_xgb %>% 
   collect_predictions() %>% 
   roc_curve(is_fraud, .pred_1) %>% 
   autoplot()
```

Unfortunately, the model still has a number of false negatives, but predicts true positives in a higher level.

